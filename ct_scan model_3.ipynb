{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "uUB4s1JKWtwz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import History \n",
    "#from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import keras\n",
    "from keras import models, regularizers, layers, optimizers, losses, metrics\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post4.tar.gz (3.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [8 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\bijin\\AppData\\Local\\Temp\\pip-install-79cll1mn\\sklearn_b99460c070bd451cb4117bc3b3728395\\setup.py\", line 10, in <module>\n",
      "      LONG_DESCRIPTION = f.read()\n",
      "    File \"C:\\Users\\bijin\\anaconda3\\envs\\tensorflow\\lib\\encodings\\cp1252.py\", line 23, in decode\n",
      "      return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n",
      "  UnicodeDecodeError: 'charmap' codec can't decode byte 0x8f in position 7: character maps to <undefined>\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m History \n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix, roc_curve, auc\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models, regularizers, layers, optimizers, losses, metrics\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3a2NudT1W0zc"
   },
   "outputs": [],
   "source": [
    "#! pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "_TG8mRNWXVCD",
    "outputId": "9258a905-024b-459b-bf95-68546890a331"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from google.colab import files\\nfiles.upload()'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from google.colab import files\n",
    "files.upload()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VRU5tugLXXBh",
    "outputId": "f7dae17c-97ac-4f5e-83de-945911bb3abe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!rm -r ~/.kaggle\\n!mkdir ~/.kaggle\\n!mv ./kaggle.json ~/.kaggle/\\n!chmod 600 ~/.kaggle/kaggle.json\\n!kaggle datasets list'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!rm -r ~/.kaggle\n",
    "!mkdir ~/.kaggle\n",
    "!mv ./kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets list\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3mD4maRXYUx",
    "outputId": "ee94d1a7-44c6-4bd2-e374-8731f0203505"
   },
   "outputs": [],
   "source": [
    "#!kaggle datasets download -d hgunraj/covidxct -p /content/drive/MyDrive/dataset --unzip --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RVq2PNDPXmyZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples  61782\n",
      "Number of testing samples 21036\n"
     ]
    }
   ],
   "source": [
    "# Total number of training images\n",
    "import os\n",
    "num_of_train_samples = 0\n",
    "train=r'C:\\Users\\bijin\\Downloads\\archive\\dataset\\train'\n",
    "val=r'C:\\Users\\bijin\\Downloads\\archive\\dataset\\val'\n",
    "for train_dataset in os.listdir(train):\n",
    "    in_folder = train + \"/\" + train_dataset \n",
    "    in_folder_list = os.listdir(in_folder)\n",
    "    num_of_train_samples = num_of_train_samples + len(in_folder_list)\n",
    "print(\"Number of training samples \",num_of_train_samples)\n",
    "\n",
    "# Total number of validation images\n",
    "num_of_validation_samples = 0\n",
    "for validation_dataset in os.listdir(val):\n",
    "    in_folder_val = val + \"/\" + validation_dataset\n",
    "    in_folder_val_list = os.listdir(in_folder_val)\n",
    "    num_of_validation_samples = num_of_validation_samples + len(in_folder_val_list)\n",
    "print(\"Number of testing samples\", num_of_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Mrjz2VwpZueW"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_oohLABsZxP1"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "target_names = ['COVID', 'PNEUMONIA', 'NORMAL']\n",
    "\n",
    "# Defining image width and height respectively\n",
    "img_rows = 224\n",
    "img_cols = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kDfHFXBwZzyU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61782 images belonging to 3 classes.\n",
      "Found 21036 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(train,\n",
    "                                                    target_size = (img_rows, img_cols),\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    class_mode = 'categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(val,\n",
    "                                                        target_size = (img_rows, img_cols),\n",
    "                                                        batch_size = batch_size,\n",
    "                                                        shuffle = False, class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "l5luXX3yZ5jF"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2018445509.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [16]\u001b[1;36m\u001b[0m\n\u001b[1;33m    This function prints and plots the confusion matrix.\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\"\"\"def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    #This function prints and plots the confusion matrix.\n",
    "    #Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print('Matriz de Confusão Não Normalizada')\n",
    "    \n",
    "    import itertools\n",
    "    print(cm)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('Classe Real')\n",
    "    plt.xlabel('Classe Predita')\n",
    "    plt.tight_layout()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vvvR1dJeaCvw"
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = num_of_train_samples // batch_size\n",
    "batch_size = 16\n",
    "epoch = 15\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "G0JWmUmXaGiz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 2048)              23587712  \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 2048)             8192      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               524544    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,121,219\n",
      "Trainable params: 529,411\n",
      "Non-trainable params: 23,591,808\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import ResNet50\n",
    "resnet_model = Sequential()\n",
    "resnet_model.add(ResNet50(include_top=False, \n",
    "                   pooling='max', \n",
    "                   weights='imagenet'))\n",
    "resnet_model.add(BatchNormalization())\n",
    "resnet_model.add(Flatten())\n",
    "resnet_model.add(Dense(256, activation = \"relu\"))\n",
    "resnet_model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "# Summary: to find the number of parameters\n",
    "resnet_model.layers[0].trainable=False\n",
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "s2b_0_2PaJXs"
   },
   "outputs": [],
   "source": [
    "resnet_model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpsx2FQEaLvl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "3861/3861 [==============================] - 5732s 1s/step - loss: 0.8101 - accuracy: 0.6268 - val_loss: 0.7412 - val_accuracy: 0.6824\n",
      "Epoch 2/15\n",
      "3861/3861 [==============================] - 5470s 1s/step - loss: 0.6640 - accuracy: 0.7171 - val_loss: 0.6741 - val_accuracy: 0.7176\n",
      "Epoch 3/15\n",
      "3861/3861 [==============================] - 5122s 1s/step - loss: 0.6177 - accuracy: 0.7389 - val_loss: 0.5974 - val_accuracy: 0.7525\n",
      "Epoch 4/15\n",
      "2084/3861 [===============>..............] - ETA: 2:03:15 - loss: 0.5941 - accuracy: 0.7506"
     ]
    }
   ],
   "source": [
    "resnet_training = resnet_model.fit(train_generator,\n",
    "                               steps_per_epoch = steps_per_epoch,\n",
    "                               epochs = epoch,\n",
    "                               validation_data = validation_generator,\n",
    "                               validation_steps = num_of_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WZ47ScPaOQ3"
   },
   "outputs": [],
   "source": [
    "training_accuracy_resnet      = resnet_training.history['accuracy'][-1]\n",
    "training_loss_resnet          = resnet_training.history['loss'][-1]\n",
    "validation_accuracy_resnet    = resnet_training.history['val_accuracy'][-1]\n",
    "validation_loss_resnet        = resnet_training.history['val_loss'][-1]\n",
    "print(\"Training Accuracy ResNet   :\", training_accuracy_resnet)\n",
    "print(\"Training Loss ResNet       :\", training_loss_resnet)\n",
    "print(\"Validation Accuracy ResNet :\", validation_accuracy_resnet)\n",
    "print(\"Validation Loss ResNet     :\", validation_loss_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
